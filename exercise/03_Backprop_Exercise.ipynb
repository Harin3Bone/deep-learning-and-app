{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: blue;\">ให้กรอกรหัสนิสิต และชื่อ-นามสกุลของสมาชิกในทีม</span>\n",
    "# <span style=\"color: blue;\">(ห้ามมีสมาชิกเกิน 2 คน)</span>\n",
    "## รหัสนิสิต ชื่อ-นามสกุล"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation Exercise\n",
    "In this exercise we will use backpropagation to train a multi-layer perceptron (with a single hidden layer).  We will experiment with different patterns and see how quickly or slowly the weights converge.  We will see the impact and interplay of different parameters such as learning rate, number of iterations, and number of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preliminaries\n",
    "from __future__ import division, print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill out the code below so that it creates a multi-layer perceptron with a single hidden layer (with 4 nodes) and trains it via back-propagation.  Specifically your code should:\n",
    "\n",
    "1. Initialize the weights to random values between -1 and 1\n",
    "1. Perform the feed-forward computation\n",
    "1. Compute the loss function\n",
    "1. Calculate the gradients for all the weights via back-propagation\n",
    "1. Update the weight matrices (using a learning_rate parameter)\n",
    "1. Execute steps 2-5 for a fixed number of iterations\n",
    "1. Plot the accuracies and log loss and observe how they change over time\n",
    "\n",
    "\n",
    "Once your code is running, try it for the different patterns below.\n",
    "\n",
    "- Which patterns was the neural network able to learn quickly and which took longer?\n",
    "- What learning rates and numbers of iterations worked well?\n",
    "- If you have time, try varying the size of the hidden layer and experiment with different activation functions (e.g. ReLu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## This code below generates two x values and a y value according to different patterns\n",
    "## It also creates a \"bias\" term (a vector of 1s)\n",
    "## The goal is then to learn the mapping from x to y using a neural network via back-propagation\n",
    "\n",
    "np.random.seed(12345)\n",
    "\n",
    "num_obs = 4000\n",
    "x_mat_1 = np.random.uniform(-1, 1, size=(num_obs, 2))\n",
    "x_mat_bias = np.ones((num_obs, 1))\n",
    "x_mat_full = np.concatenate((x_mat_1, x_mat_bias), axis=1)\n",
    "\n",
    "# 5-petal flower shape\n",
    "r = np.sqrt(x_mat_full[:, 0] ** 2 + x_mat_full[:, 1] ** 2)\n",
    "theta = np.arctan2(x_mat_full[:, 1], x_mat_full[:, 0])\n",
    "y = (r < (0.7 + 0.3 * np.sin(5 * theta))).astype(int)\n",
    "\n",
    "print('shape of x_mat_full is {}'.format(x_mat_full.shape))\n",
    "print('shape of y is {}'.format(y.shape))\n",
    "\n",
    "fig,  ax = plt.subplots(figsize=(8, 8))\n",
    "ax.plot(x_mat_full[y==1, 0], x_mat_full[y==1, 1], 'x', label='class 1', color='chocolate')\n",
    "ax.plot(x_mat_full[y==0, 0], x_mat_full[y==0, 1], 'o', label='class 0', color='darkslateblue')\n",
    "ax.grid(True)\n",
    "ax.legend(loc='best', bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Sigmoid function\n",
    "    \"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def loss_fn(y_true, y_pred, eps=1e-16):\n",
    "    \"\"\"\n",
    "    Loss function we would like to optimize (minimize)\n",
    "    We are using Logarithmic Loss\n",
    "    http://scikit-learn.org/stable/modules/model_evaluation.html#log-loss\n",
    "    \"\"\"\n",
    "    y_pred = np.maximum(y_pred, eps)\n",
    "    y_pred = np.minimum(y_pred, (1-eps))\n",
    "    return -(np.sum(y_true*np.log(y_pred)) + np.sum((1-y_true)*np.log(1-y_pred)))/len(y_true)\n",
    "\n",
    "def forward_pass(W_1, W_2, x_mat, y):\n",
    "    \"\"\"\n",
    "    Does a forward computation of the neural network\n",
    "    Takes the input `x_mat` and produces the output `y_pred`\n",
    "    Also produces the gradient of the log loss function\n",
    "    \"\"\"\n",
    "    # First,  compute the new predictions `y_pred`\n",
    "    z_2 = np.dot(x_mat, W_1)\n",
    "    a_2 = sigmoid(z_2)\n",
    "    z_3 = np.dot(a_2, W_2)\n",
    "    y_pred = sigmoid(z_3).reshape((len(x_mat), ))\n",
    "    # Now compute the gradient\n",
    "    J_z_3_grad = y_pred - y\n",
    "    J_W_2_grad = np.dot(J_z_3_grad, a_2).reshape(-1, 1)\n",
    "    a_2_z_2_grad = sigmoid(z_2)*(1-sigmoid(z_2))\n",
    "    J_W_1_grad = (np.dot((J_z_3_grad).reshape(-1, 1), W_2.reshape(-1, 1).T)*a_2_z_2_grad).T.dot(x_mat).T\n",
    "    gradient = (J_W_1_grad, J_W_2_grad)\n",
    "    \n",
    "    return y_pred, gradient\n",
    "\n",
    "def plot_loss_accuracy(loss_vals, accuracies):\n",
    "    fig = plt.figure(figsize=(16, 7))\n",
    "    fig.suptitle('Log Loss and Accuracy over iterations')\n",
    "    \n",
    "    epochs = np.arange(1, len(loss_vals) + 1)\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    ax.plot(epochs, loss_vals)\n",
    "    ax.grid(True)\n",
    "    ax.set(xlabel='epochs', title='Log Loss')\n",
    "    ax.set_xticks([1] + list(ax.get_xticks()[ax.get_xticks() > 1]))\n",
    "    \n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    ax.plot(epochs, accuracies)\n",
    "    ax.grid(True)\n",
    "    ax.set(xlabel='epochs', title='Accuracy')\n",
    "    ax.set_xticks([1] + list(ax.get_xticks()[ax.get_xticks() > 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <span style=\"color: green;\">1. ปรับเพิ่มโค้ดใน Cell ด้านล่างเพื่อให้โมเดลเรียนรู้จากข้อมูลและแสดงให้เห็นว่าโมเดลให้ค่า accuracy ไม่น้อยกว่า 70%</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Initialize the network parameters\n",
    "np.random.seed(12345)\n",
    "\n",
    "W_1 = np.random.uniform(-1, 1, (x_mat_full.shape[-1], 3))\n",
    "W_2 = \n",
    "num_epochs = \n",
    "learning_rate = \n",
    "x_mat = x_mat_full\n",
    "\n",
    "loss_vals, accuracies = [], []\n",
    "for i in range(1, num_epochs+1):\n",
    "    ### Do a forward computation, and get the gradient\n",
    "    \n",
    "    ## Update the weight matrices\n",
    "    \n",
    "    ### Compute the loss and accuracy\n",
    "\n",
    "    ## Print the loss and accuracy for every 200th iteration\n",
    "    \n",
    "plot_loss_accuracy(loss_vals, accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: green;\">2. ปรับโครงสร้างโมเดลและค่า hyperparameter ที่เห็นสมควร พร้อมอธิบายสิ่งที่ได้ปรับและเหตุผล เพื่อให้โมเดลมีประสิทธิภาพที่ให้ค่า accuracy ไม่น้อยกว่า 95%</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: green;\">3. ให้วาดกราฟของคลาส Positive และ Negative ที่ทำนายถูกและไม่ถูก และให้แสดง Confustion Matrix ดังตัวอย่างในรูป และให้เขียนอธิบายสิ่งที่สังเกตได้จากกราฟทั้งสอง</span>\n",
    "![](https://drive.google.com/thumbnail?id=156xTSDBMlhHJBlVnBG2KvOeUt-reneE0&sz=w642)\n",
    "![](https://drive.google.com/thumbnail?id=16lVXq3JWbGlw_QRx1_YWBQvMXL0DdsjH&sz=w482)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: green;\">4. ใช้โมเดลที่เทรนได้ค่า accuracy ตามที่โจทย์กำหนด ให้แสดงรายการข้อมูลที่โมเดลทำนายผิดของแต่ละคลาสมาอย่างละ 3 รายการ หรือหากมีจำนวนน้อยกว่า ให้แสดงเท่าที่มี</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_wsl_364",
   "language": "python",
   "name": "dl_wsl_364"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
